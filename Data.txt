OWN MODEL:
Validation Accuracy: [29.125  30.125  31.0625 30.0625 30.25   30.125  29.8125 29.4375 29.1875
 28.875 ]
Validation Loss: [3123.7173569  3092.65682602 3105.57458132 3135.86255589 3195.20758265
 3245.73616835 3301.36144431 3369.82866059 3441.17619515 3513.53805758]
Classwise Accuracy: [13.46153846 23.92638037 40.50632911 14.28571429 22.92993631 24.28571429
 26.55367232 29.29936306 56.97674419 33.96226415]
Training Loss: [65348.02695685 57683.60591616 53904.73826768 50876.41744986
 48214.57130942 45793.28823602 43546.76741269 41493.4260717
 39601.88947688 37748.10209007]





FIXED::------------------
LR = 0.001, epochs = 40
VAL ACC = 0.6490
CLASSWISE  = [0.64077669 0.76404494 0.43000001 0.48543689 0.5        0.53488374
  0.8214286  0.69607842 0.83018869 0.73394495]
Train loss = tensor([4.8653, 3.6573, 3.1862, 2.8779, 2.6409, 2.4466, 2.2700, 2.1135, 1.9730,
        1.8376, 1.7420, 1.6335, 1.5481, 1.4443, 1.3547, 1.3146, 1.2454, 1.1907,
        1.1475, 1.0778, 0.9883, 0.9565, 0.8862, 0.8662, 0.8134, 0.7863, 0.7771,
        0.7353, 0.6883, 0.6879, 0.6746, 0.6109, 0.6046, 0.5638, 0.5293, 0.5496,
        0.5270, 0.4853, 0.4909, 0.5053])
Val loss = tensor([0.1260, 0.1058, 0.0973, 0.0908, 0.0889, 0.0886, 0.0931, 0.0991, 0.1038,
        0.1157, 0.1245, 0.1341, 0.1328, 0.1441, 0.1563, 0.1432, 0.1534, 0.1530,
        0.1569, 0.1685, 0.1889, 0.1764, 0.1897, 0.1949, 0.2098, 0.2122, 0.2234,
        0.2219, 0.2264, 0.2453, 0.2343, 0.2533, 0.2587, 0.2688, 0.2705, 0.2745,
        0.2731, 0.2943, 0.2705, 0.2840])
Train accs = tensor([0.5720, 0.6515, 0.6935, 0.7135, 0.7445, 0.7675, 0.7685, 0.7675, 0.7785,
        0.7865, 0.8365, 0.8225, 0.7910, 0.8280, 0.8450, 0.8425, 0.8295, 0.8370,
        0.8660, 0.8795, 0.8715, 0.8870, 0.8805, 0.8845, 0.8790, 0.8970, 0.9090,
        0.8815, 0.9205, 0.9160, 0.9020, 0.9175, 0.9260, 0.9190, 0.9255],
       dtype=torch.float64)
tensor([0.5605, 0.6355, 0.6760, 0.6795, 0.6850, 0.6825, 0.6800, 0.6700, 0.6680,
        0.6690, 0.6700, 0.6655, 0.6445, 0.6510, 0.6570, 0.6515, 0.6375, 0.6415,
        0.6470, 0.6480, 0.6455, 0.6385, 0.6500, 0.6575, 0.6320, 0.6475, 0.6440,
        0.6425, 0.6565, 0.6580, 0.6430, 0.6510, 0.6580, 0.6490, 0.6515],
       dtype=torch.float64)


LR =0.1, epochs = 20
Val acc= 0.1
Classwise = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
Train loss = tensor([69.0353,  7.2338,  7.2340,  7.2340,  7.2340,  7.2340,  7.2340,  7.2340,
         7.2340,  7.2340,  7.2340,  7.2340,  7.2340,  7.2340,  7.2340,  7.2340,
         7.2340,  7.2340,  7.2340,  7.2340])
Val loss = tensor([0.2322, 0.2322, 0.2322, 0.2322, 0.2322, 0.2322, 0.2322, 0.2322, 0.2322,
        0.2322, 0.2322, 0.2322, 0.2322, 0.2322, 0.2322, 0.2322, 0.2322, 0.2322,
        0.2322, 0.2322])


LR = 0.00001, epochs = 15
Val acc= (0.4850)
Classwise = [0.407767   0.75280899 0.25999999 0.35922331 0.23333333 0.41860464
  0.65178573 0.54901963 0.60377359 0.57798165]
Train loss = tensor([6.6126, 5.6835, 5.4495, 5.3000, 5.1790, 5.0811, 5.0014, 4.9343, 4.8771,
        4.8264, 4.7791, 4.7340, 4.6905, 4.6481, 4.6070])
Val loss = tensor([0.1902, 0.1789, 0.1734, 0.1692, 0.1657, 0.1630, 0.1606, 0.1585, 0.1567,
        0.1551, 0.1535, 0.1520, 0.1506, 0.1493, 0.1480])



SCHEDULER:-----------------
LR = 0.1-0.01, epochs = 20
VAL ACC = tensor(0.1060)
CLASSWISE = tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])
Train loss = tensor([278.6255,   7.2319,   7.2307,   7.2292,   7.2276,   7.2259,   7.2243,
          7.2227,   7.2210,   7.2194,   7.2177,   7.2161,   7.2144,   7.2127,
          7.2109,   7.2092,   7.2074,   7.2056,   7.2038,   7.2021])
Val loss = tensor([2.3222, 2.3219, 2.3219, 2.3218, 2.3216, 2.3213, 2.3208, 2.3201, 2.3193,
        2.3183, 2.3171, 2.3157, 2.3141, 2.3123, 2.3104, 2.3085, 2.3066, 2.3049,
        2.3035, 2.3026])


KL DIVERGNECE LOSS:
Effects to be noted in terms of regularisation, and avoiding overfitting
In some cases, KL divergence loss may require a higher learning rate than cross-entropy loss. 
This is because KL divergence loss can be a more challenging optimization problem than cross-entropy loss,
particularly when it is used as a regularization term.
A higher learning rate can help the optimization process converge faster and may lead to better performance.

LR = 0.001
train loss = tensor([92.0053, 91.5795, 91.4057, 91.2935, 91.2126, 91.1497, 91.0966, 91.0497,
        91.0058, 90.9676, 90.9355, 90.9113, 90.8894, 90.8633, 90.8418])
validation loss = tensor([0.9784, 0.9760, 0.9752, 0.9749, 0.9749, 0.9749, 0.9753, 0.9755, 0.9755,
        0.9758, 0.9765, 0.9773, 0.9777, 0.9779, 0.9774])
Classwise =  [0.53398061 0.77528089 0.47       0.55339807 0.5        0.58139533
  0.6785714  0.74509805 0.75471699 0.74311924]

Accuracy val = tensor(0.6360)


WITHOUT DATA AUGMENT:
val acc = tensor(0.6600)
classwise = tensor([0.6505, 0.7303, 0.5600, 0.5146, 0.5889, 0.5581, 0.8036, 0.7255, 0.7170,
        0.7156])
train loss = tensor([4.8246, 3.6055, 3.0769, 2.7267, 2.4824, 2.2848, 2.1138, 1.9614, 1.8277,
        1.6934, 1.5592, 1.4498, 1.3824, 1.2627, 1.1877, 1.1134, 1.0202, 0.9460,
        0.9089, 0.8645])
val loss = tensor([0.1227, 0.1040, 0.0914, 0.0885, 0.0868, 0.0899, 0.0893, 0.0931, 0.1054,
        0.1075, 0.1112, 0.1173, 0.1233, 0.1221, 0.1321, 0.1406, 0.1402, 0.1554,
        0.1771, 0.1696]

accuracy: 
train acc:
[0.57450002 0.65450001 0.69099998 0.71499997 0.72500002 0.7475
 0.7705     0.78600001 0.79949999 0.792      0.80650002 0.801
 0.815      0.83050001 0.82749999 0.82599998 0.833      0.83600003
 0.83050001 0.86849999]
validation acc:
[0.565      0.63050002 0.65700001 0.68550003 0.69       0.69199997
 0.69349998 0.69950002 0.69400001 0.68849999 0.68199998 0.67750001
 0.68550003 0.67400002 0.66949999 0.67799997 0.67500001 0.67449999
 0.67750001 0.671     ]


BATCH SIZE:
Analysis: Higher batch size ---> Lower test accuracy
Lower batch size ---> Higher test accuracy (More training time, more number of gradient descents,
                      smaller but more accurate gradient descents)
But for complex models, very small batch size like 2-4 would show poor results.
32
[0.5605, 0.6355, 0.6760, 0.6795, 0.6850, 0.6825, 0.6800, 0.6700, 0.6680,
        0.6690, 0.6700, 0.6655, 0.6445, 0.6510, 0.6570, 0.6515, 0.6375, 0.6415,
        0.6470, 0.6480, 0.6455, 0.6385, 0.6500, 0.6575, 0.6320, 0.6475, 0.6440,
        0.6425, 0.6565, 0.6580, 0.6430, 0.6510, 0.6580, 0.6490, 0.6515]

16
[0.5800, 0.6610, 0.6920, 0.6930, 0.6870, 0.6850, 0.6870, 0.6840, 0.6760,
        0.6750, 0.6840, 0.6920, 0.6800, 0.6860, 0.6840, 0.6880, 0.6800, 0.6780,
        0.6700, 0.6350]


8
[0.5780, 0.6500, 0.6740, 0.6920, 0.6870, 0.6860, 0.6700, 0.6540, 0.6700,
        0.6370, 0.6620, 0.6470, 0.6450, 0.6300, 0.6510, 0.6470, 0.6300, 0.6650,
        0.6480, 0.6410]



4
[0.5620, 0.6330, 0.6570, 0.6590, 0.6540, 0.6400, 0.6330, 0.6370, 0.6560,
        0.6420, 0.6350, 0.6240, 0.6450, 0.6330, 0.6520, 0.6500, 0.6360, 0.6340,
        0.6400, 0.6310]



